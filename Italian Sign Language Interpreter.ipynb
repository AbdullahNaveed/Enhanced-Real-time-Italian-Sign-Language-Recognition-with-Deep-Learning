{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217d3af2",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9034beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_29368\\3130913924.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam opened. Press 'x' to exit.\n",
      "Predicted letter: H\n",
      "Predicted letter: Y\n",
      "Predicted letter: W\n",
      "Predicted letter: Q\n",
      "User pressed 'x'. Exiting...\n",
      "Final recognized word: HYWQ\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "##############################\n",
    "#                            #\n",
    "#  CNN Model Architecture    #\n",
    "#                            #\n",
    "##############################\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, numClasses=22):\n",
    "        \n",
    "        super(DeepCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            # Block#1: 3 -> 32\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block#2: 32 -> 64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block#3: 64 -> 128\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block#4: 128 -> 256\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block#5: 256 -> 512\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, numClasses)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)           \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#                                                     #\n",
    "#  Setting up Device, Model, Classes and Transform    #\n",
    "#                                                     #\n",
    "#######################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "numClasses = 22\n",
    "model = DeepCNN(numClasses=numClasses).to(device)\n",
    "\n",
    "#loading the model\n",
    "model_path = \"best_deepcnn.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "#class names\n",
    "classNames = [\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \n",
    "    \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \n",
    "    \"O\", \"P\", \"Q\", \"R\", \"T\", \"U\", \n",
    "    \"V\", \"W\", \"X\", \"Y\"\n",
    "]\n",
    "\n",
    "#image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "#####################################\n",
    "#                                   #\n",
    "#  Prediction of Image with Model   #\n",
    "#                                   #\n",
    "#####################################\n",
    "\n",
    "def predict_letter(model, frameBgr, device):\n",
    "\n",
    "    #converting BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(frameBgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #converting to a PIL Image\n",
    "    pil_img = Image.fromarray(img_rgb)\n",
    "    \n",
    "    #applying transforms\n",
    "    img_t = transform(pil_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        _, predicted_idx = torch.max(outputs, 1)\n",
    "        \n",
    "    return classNames[predicted_idx.item()]\n",
    "\n",
    "#####################################\n",
    "#                                   #\n",
    "#  Webcam Access and Processing     #\n",
    "#                                   #\n",
    "#####################################\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Webcam opened. Press 'x' to exit.\")\n",
    "\n",
    "recognizedWord = \"\"\n",
    "capture_interval = 3\n",
    "start_time = time.time()\n",
    "box_size = 224\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read from webcam.\")\n",
    "        break\n",
    "\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    \n",
    "    #computing center bounding box coordinates\n",
    "    center_x = frame_width//2\n",
    "    center_y = frame_height//2\n",
    "    x1 = center_x-(box_size//2)\n",
    "    y1 = center_y-(box_size//2)\n",
    "    x2 = x1+box_size\n",
    "    y2 = y1+box_size\n",
    "\n",
    "    #drawing a bounding box on the frame\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    #displaying the recognized word so far\n",
    "    cv2.putText(frame, f\"Word: {recognizedWord}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "\n",
    "    #displaying an info text on the frame\n",
    "    cv2.putText(frame, \"Image every 3s. Press 'x' to exit.\", (10, frame_height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    #checking if the capture interval has elapsed\n",
    "    current_time = time.time()\n",
    "    if (current_time - start_time) >= capture_interval:\n",
    "        #reseting the timer\n",
    "        start_time = current_time\n",
    "\n",
    "        #croping the center 224x224 region\n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        #pedicting the letter using the model\n",
    "        letter = predict_letter(model, roi, device)\n",
    "        print(\"Predicted letter:\", letter)\n",
    "        recognizedWord += letter\n",
    "\n",
    "    #displaying the webcam feed\n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "\n",
    "    #exitting if user presses 'x' or 'X'\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('x') or key == ord('X'):\n",
    "        print(\"User pressed 'x'. Exiting...\")\n",
    "        break\n",
    "\n",
    "#cleaning up resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Final recognized word:\", recognizedWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c77da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8e050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
